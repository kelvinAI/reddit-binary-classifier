{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Statement:\" data-toc-modified-id=\"Problem-Statement:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Problem Statement:</a></span></li><li><span><a href=\"#Scraping-for-data\" data-toc-modified-id=\"Scraping-for-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scraping for data</a></span></li><li><span><a href=\"#Read-the-csv-file\" data-toc-modified-id=\"Read-the-csv-file-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Read the csv file</a></span></li><li><span><a href=\"#Baseline-accuracy\" data-toc-modified-id=\"Baseline-accuracy-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Baseline accuracy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Our-baseline-accuracy-is-as-above-for-each-class\" data-toc-modified-id=\"Our-baseline-accuracy-is-as-above-for-each-class-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Our baseline accuracy is as above for each class</a></span></li></ul></li><li><span><a href=\"#NLP-Pipeline\" data-toc-modified-id=\"NLP-Pipeline-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>NLP Pipeline</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Train test split</a></span></li><li><span><a href=\"#Count-Vectorizer-pipeline\" data-toc-modified-id=\"Count-Vectorizer-pipeline-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Count Vectorizer pipeline</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "Scape the Reddit website for two topics, and build a binary classifier that will classify which topic the text originate from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def extract_content(response, topic):\n",
    "    # 0th index refers to title appended with selftext\n",
    "    rslt = defaultdict(list)\n",
    "    for post in response.json()['data']['children']:\n",
    "        rslt['content'].append(post['data']['title'] + post['data']['selftext'])\n",
    "        rslt['topic'].append(topic)\n",
    "        rslt['id'].append(post['data']['name'])\n",
    "        \n",
    "    return rslt\n",
    "\n",
    "def scrape_topic(topic,depth=12):\n",
    "    '''\n",
    "    Scrape the reddit topic for all the posts\n",
    "    \n",
    "    '''\n",
    "    base_url = 'https://www.reddit.com/r/'\n",
    "    url = base_url + topic + '.json?limit=100'\n",
    "    last_entry_name = ''\n",
    "    payload = None\n",
    "    \n",
    "    \n",
    "    for i in range(depth):\n",
    "        if last_entry_name == None:\n",
    "            break\n",
    "        if last_entry_name != '':\n",
    "            url = base_url + topic + '.json?limit=100&after=' + last_entry_name\n",
    "    \n",
    "        res = requests.get(url, headers={'User-agent': 'Pony Inc 1.0'})\n",
    "        print(f\"request url: {url} for {topic} at depth:{i}\")\n",
    "        \n",
    "        last_entry_name = res.json()['data']['after']\n",
    "        print(f\"Last entry name : {last_entry_name} \")\n",
    "        content = extract_content(res,topic)\n",
    "        if not payload:\n",
    "            payload = content\n",
    "        else:\n",
    "            for k,v in payload.items():\n",
    "                payload[k] += content[k]\n",
    "        if i < depth-1:\n",
    "            sleep_duration = random.randint(2,30)\n",
    "            time.sleep(sleep_duration)\n",
    "        print(f'Scraping {topic}, {i} pages in..')\n",
    "    return {'topic': topic, 'data': payload }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as cf\n",
    "import random, time\n",
    "\n",
    "\n",
    "def get_reddit_data(topics):\n",
    "    '''\n",
    "    Main function to scrape reddit data into a pandas dataframe.\n",
    "    Args: topics\n",
    "     - An array object of topics in reddit to scrape from\n",
    "    '''\n",
    "    rdf = pd.DataFrame()\n",
    "    with cf.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        for future in executor.map(scrape_topic, topics):\n",
    "            if rdf.empty:\n",
    "                rdf = pd.DataFrame(future['data'])\n",
    "            else:\n",
    "                new = pd.DataFrame(future['data'])\n",
    "                rdf = pd.concat([rdf,new],ignore_index=True)\n",
    "            \n",
    "    return rdf\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100 for legaladvice at depth:0\n",
      "Last entry name : t3_dkz2et \n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100 for AskDocs at depth:0\n",
      "Last entry name : t3_dkv1dd \n",
      "Scraping legaladvice, 0 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_dkz2et for legaladvice at depth:1\n",
      "Last entry name : t3_dklq67 \n",
      "Scraping legaladvice, 1 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_dklq67 for legaladvice at depth:2\n",
      "Last entry name : t3_dkqwo4 \n",
      "Scraping AskDocs, 0 pages in..\n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100&after=t3_dkv1dd for AskDocs at depth:1\n",
      "Last entry name : t3_dkrf1i \n",
      "Scraping legaladvice, 2 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_dkqwo4 for legaladvice at depth:3\n",
      "Last entry name : t3_dkbfqt \n",
      "Scraping AskDocs, 1 pages in..\n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100&after=t3_dkrf1i for AskDocs at depth:2\n",
      "Last entry name : t3_dknx6c \n",
      "Scraping legaladvice, 3 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_dkbfqt for legaladvice at depth:4\n",
      "Last entry name : t3_dkgoj8 \n",
      "Scraping AskDocs, 2 pages in..\n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100&after=t3_dknx6c for AskDocs at depth:3\n",
      "Last entry name : t3_dkgcih \n",
      "Scraping AskDocs, 3 pages in..\n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100&after=t3_dkgcih for AskDocs at depth:4\n",
      "Last entry name : t3_dk7p9l \n",
      "Scraping legaladvice, 4 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_dkgoj8 for legaladvice at depth:5\n",
      "Last entry name : t3_dkc5y7 \n",
      "Scraping legaladvice, 5 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_dkc5y7 for legaladvice at depth:6\n",
      "Last entry name : t3_dkbve0 \n",
      "Scraping AskDocs, 4 pages in..\n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100&after=t3_dk7p9l for AskDocs at depth:5\n",
      "Last entry name : t3_dk8odz \n",
      "Scraping legaladvice, 6 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_dkbve0 for legaladvice at depth:7\n",
      "Last entry name : t3_djnpm4 \n",
      "Scraping legaladvice, 7 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_djnpm4 for legaladvice at depth:8\n",
      "Last entry name : t3_djucel \n",
      "Scraping AskDocs, 5 pages in..\n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100&after=t3_dk8odz for AskDocs at depth:6\n",
      "Last entry name : t3_dk2bq7 \n",
      "Scraping legaladvice, 8 pages in..\n",
      "request url: https://www.reddit.com/r/legaladvice.json?limit=100&after=t3_djucel for legaladvice at depth:9\n",
      "Last entry name : None \n",
      "Scraping AskDocs, 6 pages in..\n",
      "Scraping legaladvice, 9 pages in..\n",
      "request url: https://www.reddit.com/r/AskDocs.json?limit=100&after=t3_dk2bq7 for AskDocs at depth:7\n",
      "Last entry name : None \n",
      "Scraping AskDocs, 7 pages in..\n",
      "             content   id\n",
      "topic                    \n",
      "AskDocs          770  770\n",
      "legaladvice      990  990\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "rdf = get_reddit_data(['AskDocs','legaladvice'])\n",
    "print(rdf.groupby(by='topic').count())\n",
    "\n",
    "# filename = 'reddit_scrape_bio_chem.csv'\n",
    "filename = 'reddit_scrape_docs_legal.csv'\n",
    "\n",
    "use_header=True\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    use_header = False\n",
    "\n",
    "# Save the data into a csv file\n",
    "rdf.to_csv(filename,mode='a',header=use_header, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>topic</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Weekly Discussion/General Questions Thread - O...</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>t3_dhnk4v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(F18) Hit my head off a nail in the wall repea...</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>t3_dktyay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[29F] Fell on ribs a month ago, xray showed no...</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>t3_dl06ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>What happened to me last night ?25M\\nWeight 73...</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>t3_dkudly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Back injury advise.26\\nMale\\n5\" 11\\n85kg\\nCauc...</td>\n",
       "      <td>AskDocs</td>\n",
       "      <td>t3_dl0nns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755</td>\n",
       "      <td>[FL] In June my wife got a speeding ticket and...</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>t3_djt05l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1756</td>\n",
       "      <td>Domestic Violence MichiganI’m seeking advice f...</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>t3_djvyvt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1757</td>\n",
       "      <td>My future ex wife is trying to get our dogs ce...</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>t3_djblj9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1758</td>\n",
       "      <td>Rented apartment is at 85% humidity. Building ...</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>t3_djsw8k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1759</td>\n",
       "      <td>North Carolina - Accrued Vacation Payout Quest...</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>t3_di8e23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1760 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content        topic  \\\n",
       "0     Weekly Discussion/General Questions Thread - O...      AskDocs   \n",
       "1     (F18) Hit my head off a nail in the wall repea...      AskDocs   \n",
       "2     [29F] Fell on ribs a month ago, xray showed no...      AskDocs   \n",
       "3     What happened to me last night ?25M\\nWeight 73...      AskDocs   \n",
       "4     Back injury advise.26\\nMale\\n5\" 11\\n85kg\\nCauc...      AskDocs   \n",
       "...                                                 ...          ...   \n",
       "1755  [FL] In June my wife got a speeding ticket and...  legaladvice   \n",
       "1756  Domestic Violence MichiganI’m seeking advice f...  legaladvice   \n",
       "1757  My future ex wife is trying to get our dogs ce...  legaladvice   \n",
       "1758  Rented apartment is at 85% humidity. Building ...  legaladvice   \n",
       "1759  North Carolina - Accrued Vacation Payout Quest...  legaladvice   \n",
       "\n",
       "             id  \n",
       "0     t3_dhnk4v  \n",
       "1     t3_dktyay  \n",
       "2     t3_dl06ul  \n",
       "3     t3_dkudly  \n",
       "4     t3_dl0nns  \n",
       "...         ...  \n",
       "1755  t3_djt05l  \n",
       "1756  t3_djvyvt  \n",
       "1757  t3_djblj9  \n",
       "1758  t3_djsw8k  \n",
       "1759  t3_di8e23  \n",
       "\n",
       "[1760 rows x 3 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any duplicates if there are any\n",
    "reddit_df = reddit_df[~reddit_df.duplicated(subset='id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove id column\n",
    "reddit_df = reddit_df[['content','topic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 2)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the shape of the dataframe\n",
    "reddit_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AskDocs</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>legaladvice</td>\n",
       "      <td>990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             content\n",
       "topic               \n",
       "AskDocs          770\n",
       "legaladvice      990"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show data from each class\n",
    "reddit_df.groupby(by='topic').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "legaladvice    0.5625\n",
       "AskDocs        0.4375\n",
       "Name: topic, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_df['topic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our baseline accuracy is as above for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a slight imbalance of the observations between the two classes, thus we need to stratify the content when splitting the data during k-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the target class\n",
    "y = reddit_df['topic'].map(lambda x : 1 if x == 'AskDocs' else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X\n",
    "X = reddit_df['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(1320,) y_train:(1320,) X_test:(440,) y_test:(440,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X_train:{X_train.shape} y_train:{y_train.shape} X_test:{X_test.shape} y_test:{y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_logreg_pipeline = Pipeline(steps=[('cvec', cvec),('logreg',logreg)])\n",
    "# Show the parameters\n",
    "# cvec_logreg_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_logreg_params = {'cvec__stop_words':['english'], 'cvec__max_df':[0.7, 1.0],'logreg__solver':['liblinear'],'logreg__C':[1,10] }\n",
    "cvec_logreg_gscv = GridSearchCV(cvec_logreg_pipeline, cvec_logreg_params, cv=5)\n",
    "cvec_logreg_gscv.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model):\n",
    "    print(f'Training Score:{model.score(X_train,y_train)}')\n",
    "    print(f'Testing Score:{model.score(X_test,y_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score:1.0\n",
      "Testing Score:0.9772727272727273\n"
     ]
    }
   ],
   "source": [
    "get_scores(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "330.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 193.20000000000002,
   "position": {
    "height": "214.8px",
    "left": "1087px",
    "right": "20px",
    "top": "84px",
    "width": "417.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
